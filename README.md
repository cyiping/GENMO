ğŸš€ NVIDIA ç™¼è¡¨ GENMOï¼šäººé¡å‹•ä½œçš„ã€Œé€šæ‰æ¨¡å‹ã€ç™»å ´ï¼
ä½ çŸ¥é“å—ï¼Ÿä»¥å‰ AI æƒ³ç”Ÿæˆè§’è‰²å‹•ä½œã€æˆ–å¾å½±ç‰‡ä¼°å‡º 3D éª¨æ¶ï¼Œè¦ç”¨å…©å€‹ä¸åŒæ¨¡å‹ã€‚
 ç¾åœ¨ NVIDIA æŠŠé€™å…©ä»¶äº‹ã€Œåˆè€Œç‚ºä¸€ã€â€”â€”æ¨å‡º GENMOï¼ˆGeneralist Model for Human Motionï¼‰ ğŸ¯
é€™æ˜¯ä¸€å€‹èƒ½ã€Œçœ‹å½±ç‰‡å°±é‚„åŸ 3D å‹•ä½œã€åˆèƒ½ã€Œæ ¹æ“šæ–‡å­—ã€éŸ³æ¨‚æˆ–é—œéµå§¿å‹¢ç”Ÿæˆå…¨æ–°å‹•ä½œã€çš„é€šç”¨æ¨¡å‹ã€‚
å®ƒåŒæ™‚æ‡‚ï¼š
ğŸ¥ å¾å½±ç‰‡ä¼°å‡ºå…¨èº« 3D å‹•ä½œèˆ‡æ”å½±æ©Ÿç§»å‹•
ğŸ’ƒ è½éŸ³æ¨‚ç”Ÿèˆè¹ˆã€çœ‹æ–‡å­—ç”Ÿå‹•ä½œ
âœ‹ ä¾ç…§ 3D é—œéµå§¿å‹¢æˆ– 2D éª¨æ¶å¼•å°
ğŸ§© ä¸åŒæ¢ä»¶å¯ä»¥ã€Œæ™‚é–“è»¸æ··æ­ã€ï¼Œè‡ªå‹•éŠœæ¥éå ´ï¼
 é€™å°æ©Ÿå™¨äººå­¸ç¿’ä¾†èªªï¼Œå®ƒå¯ä»¥æŠŠäººé¡å½±ç‰‡è®Šæˆå¯å­¸ç¿’çš„ 3D å‹•ä½œè³‡æ–™ã€‚
 é€™è®“ã€Œäººé¡å‹•ä½œç†è§£ã€èˆ‡ã€ŒAI å‹•ä½œå‰µä½œã€çµ‚æ–¼èƒ½åœ¨åŒä¸€æ¨¡å‹ä¸­å”åŒã€‚
ğŸ“˜ æ›´å¤šè³‡è¨Šï¼š
ICCV 2025 Highlight è«–æ–‡ https://research.nvidia.com/labs/dair/genmo/......

<p align="center">
 <h1 align="center"> GENMO: A Generalist Model for Human Motion</h1>
  <p align="center">
    <a href="https://jeffli.site/"><strong>Jiefeng Li</strong></a>
    Â·
    <a href="https://www.jinkuncao.com/"><strong>Jinkun Cao</strong></a>
    Â·
    <a href="https://cs.stanford.edu/~haotianz/"><strong>Haotian Zhang</strong></a>
    Â·
    <a href="https://davrempe.github.io/"><strong>Davis Rempe</strong></a>
    Â·
    <a href="https://jankautz.com/"><strong>Jan Kautz</strong></a>
    Â·
    <a href="https://www.umariqbal.info/"><strong>Umar Iqbal</strong></a>
    Â·
    <a href="https://ye-yuan.com/"><strong>Ye Yuan</strong></a>
  </p>
  <h2 align="center">ICCV 2025 (Highlight)</h2>
  <div align="center">
    <img src="./assets/teaser.png" alt="Logo" width="100%">
  </div>
</p>
<p align="center">
  <a href="https://research.nvidia.com/labs/dair/genmo/"><img src="https://img.shields.io/badge/Project-Page-0099cc"></a>
  <a href="https://arxiv.org/abs/2505.01425"><img src="https://img.shields.io/badge/arXiv-2505.01425-b31b1b.svg"></a>

</p>

**GENMO** is a generalist model for human motion that handles multiple tasks with a single model, supporting diverse conditioning signals including video, keypoints, text, audio, and 3D keyframes.

---

## ğŸ“° News

- **[October 2025]** ğŸ“¢ The **GENMO** codebase is **released!**  
  Stay tuned for the pretrained models and evaluation scripts.  
  Follow the [project page](https://research.nvidia.com/labs/dair/genmo/) for updates and announcements.


---

## ğŸš€ Highlights

GENMO introduces a **unified generative framework** that connects motion estimation and generation through shared objectives.

- **Unified framework:** Reframes motion estimation as *constrained generation*, allowing a single model to perform both tasks.  
- **Regression Ã— Diffusion synergy:** Combines the accuracy of regression models with the diversity of diffusion-based generation.  
- **Estimation-guided training:** Trains effectively on in-the-wild datasets using only 2D or textual supervision.  
- **Multimodal conditioning:** Supports video, text, audio, 2D/3D keyframes, or even time-varying mixed inputs (e.g., video â†’ text â†’ video).  
- **Arbitrary-length motion:** Generates continuous, coherent sequences of any duration in one diffusion pass.  
- **State-of-the-art performance:** Achieves leading results on diverse motion estimation and generation benchmarks.

For more details, visit the **[GENMO project page â†’](https://research.nvidia.com/labs/dair/genmo/)**

---

## ğŸ“– Paper & Citation

**Paper:**  
[GENMO: A GENeralist Model for Human MOtion](https://arxiv.org/abs/2505.01425)  
*Jiefeng Li, Jinkun Cao, Haotian Zhang, Davis Rempe, Jan Kautz, Umar Iqbal, Ye Yuan*  
ICCV, 2025

**BibTeX:**
```bibtex
@inproceedings{genmo2025,
  title     = {GENMO: A GENeralist Model for Human MOtion},
  author    = {Li, Jiefeng and Cao, Jinkun and Zhang, Haotian and Rempe, Davis and Kautz, Jan and Iqbal, Umar and Yuan, Ye},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2025}
}
